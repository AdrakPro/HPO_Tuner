2025-10-08 15:10:03.343 | INFO     | Configuration:
{
    "project": {
        "name": "HPO Paper-Based Experiment",
        "seed": 29369
    },
    "environment": {
        "data_source": {
            "dataset": "CIFAR-10",
            "path": "./model_data"
        },
        "logging": {
            "log_file": "logs/experiment.log",
            "error_log_file": "logs/error.log",
            "report_directory": "reports"
        }
    },
    "checkpoint_config": {
        "interval_per_gen": 1
    },
    "parallel_config": {
        "execution": {
            "evaluation_mode": "GPU",
            "enable_parallel": true,
            "gpu_workers": 4,
            "cpu_workers": 0,
            "gpu_cpu_performance_ratio": 16,
            "dataloader_workers": {
                "per_gpu": 8,
                "per_cpu": 1
            }
        }
    },
    "neural_network_config": {
        "input_shape": [
            3,
            32,
            32
        ],
        "output_classes": 10,
        "conv_blocks": 4,
        "fixed_parameters": {
            "activation_function": "relu",
            "base_filters": 32
        },
        "hyperparameter_space": {
            "width_scale": {
                "type": "float",
                "range": [
                    1.0,
                    2.0
                ],
                "description": "Scales the number of filters in convolutional layers"
            },
            "fc1_units": {
                "type": "enum",
                "values": [
                    512,
                    768,
                    1024
                ],
                "description": "Number of neurons in the first fully connected layer"
            },
            "dropout_rate": {
                "type": "float",
                "range": [
                    0.01,
                    0.1
                ],
                "description": "Dropout intensity in the first fully connected layer"
            },
            "optimizer_schedule": {
                "type": "enum",
                "values": [
                    "ADAMW_COSINE",
                    "ADAMW_ONECYCLE"
                ],
                "description": "Optimizer type + Learning rate scheduler"
            },
            "base_lr": {
                "type": "float",
                "range": [
                    0.001,
                    0.1
                ],
                "scale": "log",
                "description": "Base learning rate value, log scale"
            },
            "aug_intensity": {
                "type": "enum",
                "values": [
                    "MEDIUM",
                    "STRONG"
                ],
                "description": "Level of data augmentation"
            },
            "weight_decay": {
                "type": "float",
                "range": [
                    1e-05,
                    0.0001
                ],
                "scale": "log",
                "description": "L2 regularization coefficient, log scale"
            },
            "batch_size": {
                "type": "enum",
                "values": [
                    64,
                    128,
                    256
                ],
                "description": "Training batch size"
            }
        }
    },
    "nested_validation_config": {
        "enabled": true,
        "outer_k_folds": 3
    },
    "genetic_algorithm_config": {
        "genetic_operators": {
            "active": [
                "selection",
                "crossover",
                "mutation",
                "elitism"
            ],
            "selection": {
                "type": "tournament",
                "tournament_size": 3
            },
            "crossover": {
                "type": "uniform",
                "crossover_prob": 0.8
            },
            "mutation": {
                "mutation_prob_discrete": 0.15,
                "mutation_prob_categorical": 0.15,
                "mutation_sigma_continuous": 0.1,
                "mutation_prob_continuous": 0.15
            },
            "elitism_percent": 0.1
        },
        "calibration": {
            "enabled": true,
            "population_size": 48,
            "generations": 10,
            "training_epochs": 20,
            "data_subset_percentage": 0.2,
            "mutation_decay_rate": 0.98,
            "stratification_bins": 9,
            "stop_conditions": {
                "max_generations": 10,
                "early_stop_generations": 999,
                "early_stop_epochs": 3,
                "fitness_goal": 0.81,
                "time_limit_minutes": 1440
            }
        },
        "main_algorithm": {
            "population_size": 24,
            "generations": 70,
            "training_epochs": 200,
            "mutation_decay_rate": 0.98,
            "stratification_bins": 3,
            "stop_conditions": {
                "max_generations": 70,
                "early_stop_generations": 20,
                "early_stop_epochs": 15,
                "fitness_goal": 0.99,
                "time_limit_minutes": 0
            }
        }
    }
}
2025-10-08 15:10:03.344 | INFO     | Logger initialized. Log file at logs/log_2025-10-08_13-10-03.log
2025-10-08 15:10:03.344 | INFO     | --- Starting Nested Resampling With 3 Folds ---
2025-10-08 15:10:04.205 | INFO     | --- Running Fold 1/3 ---
2025-10-08 15:10:04.206 | INFO     | Initializing worker pool...
2025-10-08 15:10:04.206 | INFO     | Using GPU-Only scheduling strategy.
2025-10-08 15:10:04.210 | INFO     | Worker pool initialized with 4 workers.
2025-10-08 15:10:05.308 | INFO     | --- Starting CALIBRATION Phase ---
2025-10-08 15:10:05.309 | INFO     | Generations: 10, Population: 48
2025-10-08 15:10:05.309 | WARNING  | Progressive epochs are disabled! To enable progression minimal training epochs must be at least (20).
2025-10-08 15:10:05.309 | INFO     | --- CALIBRATION - Generation 1/10 ---
2025-10-08 15:12:00.742 | INFO     | [Worker-3 / GPU-3] Evaluating Individual 3/48 (20 epochs)
2025-10-08 15:12:00.742 | INFO     | [Worker-3 / GPU-3] Hyperparameters: {'width_scale': 1.1759916712681808, 'fc1_units': 512, 'dropout_rate': 0.05146423752577828, 'optimizer_schedule': 'ADAMW_COSINE', 'base_lr': 0.04131475473818087, 'aug_intensity': 'STRONG', 'weight_decay': 8.848952150219614e-05, 'batch_size': 256}
2025-10-08 15:12:00.742 | INFO     | [Worker-3 / GPU-3] Using base_lr 0.041315 for batch_size 256
2025-10-08 15:12:00.743 | INFO     | [Worker-3 / GPU-3] Epoch 1/20 | Train Acc: 0.0984, Train Loss: 23.1124 | Test Loss: 2.3070, Test Acc: 0.1000
2025-10-08 15:12:00.743 | INFO     | [Worker-3 / GPU-3] Epoch 2/20 | Train Acc: 0.0999, Train Loss: 2.3069 | Test Loss: 2.3049, Test Acc: 0.1000
2025-10-08 15:12:00.743 | INFO     | [Worker-3 / GPU-3] Epoch 3/20 | Train Acc: 0.1006, Train Loss: 2.3059 | Test Loss: 2.3052, Test Acc: 0.1000
2025-10-08 15:12:00.743 | INFO     | [Worker-3 / GPU-3] Epoch 4/20 | Train Acc: 0.1010, Train Loss: 2.3063 | Test Loss: 2.3085, Test Acc: 0.1000
2025-10-08 15:12:00.743 | INFO     | [Worker-3 / GPU-3] Epoch 5/20 | Train Acc: 0.0964, Train Loss: 2.3066 | Test Loss: 2.3072, Test Acc: 0.1000
2025-10-08 15:12:00.743 | INFO     | [Worker-3 / GPU-3] Epoch 5/20 | Train Acc: 0.0964, Train Loss: 2.3066 | Test Loss: 2.3072, Test Acc: 0.1000 / Early stopping triggered
2025-10-08 15:12:00.744 | INFO     | [Worker-3 / GPU-3] Individual 3 -> Accuracy: 0.1000, Loss: 2.3072 | Duration: 111.55s
2025-10-08 15:12:00.744 | INFO     | [Worker-3 / GPU-3] GPU memory used: 0.06 GB
